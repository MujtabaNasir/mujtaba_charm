{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "%matplotlib inline\n",
    "\n",
    "def create_imbalanced_mnist(dataset, min_per_class=30, max_per_class=100):\n",
    "    \"\"\"\n",
    "    Creates a subset of the MNIST dataset with a specified imbalance in the number of samples per class.\n",
    "\n",
    "    The resulting subset will contain a minimum and maximum number of samples per class, where classes with\n",
    "    fewer samples than the minimum are excluded, and classes with more samples are randomly downsampled to the maximum.\n",
    "\n",
    "    Args:\n",
    "        dataset (torchvision.datasets.MNIST): The original MNIST dataset.\n",
    "        min_per_class (int, optional): Minimum number of samples per class. Classes with fewer samples are excluded. Defaults to 30.\n",
    "        max_per_class (int, optional): Maximum number of samples per class. Classes with more samples are randomly downsampled. Defaults to 100.\n",
    "\n",
    "    Returns:\n",
    "        torch.utils.data.Subset: A subset of the MNIST dataset with the specified class imbalance.\n",
    "    \"\"\"\n",
    "\n",
    "    indices = []\n",
    "    for label in range(10):\n",
    "        class_indices = np.where(np.array(dataset.targets) == label)[0]\n",
    "        if len(class_indices) > max_per_class:\n",
    "            class_indices = np.random.choice(class_indices, max_per_class, replace=False)\n",
    "        elif len(class_indices) < min_per_class:\n",
    "            continue  \n",
    "        indices.extend(class_indices)\n",
    "    return Subset(dataset, indices)\n",
    "\n",
    "transformation = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "full_train_dataset = datasets.MNIST('data/', train=True, transform=transformation, download=True)\n",
    "full_test_dataset = datasets.MNIST('data/', train=False, transform=transformation, download=True)\n",
    "\n",
    "imbalanced_train_dataset = create_imbalanced_mnist(full_train_dataset)\n",
    "imbalanced_test_dataset = create_imbalanced_mnist(full_test_dataset)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(imbalanced_train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(imbalanced_test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual block for a ResNet-like architecture.\n",
    "\n",
    "    This block consists of two convolutional layers with batch normalization and ReLU activations, \n",
    "    and includes a skip connection (residual connection) to improve gradient flow and facilitate learning.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): The number of input channels.\n",
    "        out_channels (int): The number of output channels.\n",
    "        stride (int, optional): The stride of the convolutional layers. Defaults to 1.\n",
    "\n",
    "    Attributes:\n",
    "        conv1 (torch.nn.Conv2d): The first convolutional layer.\n",
    "        bn1 (torch.nn.BatchNorm2d): The first batch normalization layer.\n",
    "        relu (torch.nn.ReLU): The ReLU activation function.\n",
    "        conv2 (torch.nn.Conv2d): The second convolutional layer.\n",
    "        bn2 (torch.nn.BatchNorm2d): The second batch normalization layer.\n",
    "        downsample (torch.nn.Sequential): The downsampling layer if needed to match dimensions.\n",
    "\n",
    "    Methods:\n",
    "        forward(x):\n",
    "            Defines the forward pass of the residual block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        \"\"\"\n",
    "        Initializes the residual block layers.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels.\n",
    "            out_channels (int): The number of output channels.\n",
    "            stride (int, optional): The stride of the convolutional layers. Defaults to 1.\n",
    "        \"\"\"\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.downsample = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the residual block.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor with shape (N, in_channels, H, W), where N is the batch size, \n",
    "                              and H and W are the height and width of the input.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor after passing through the residual block.\n",
    "        \"\"\"\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += self.downsample(identity)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    A custom neural network combining a modified ResNet18 backbone with additional residual blocks.\n",
    "\n",
    "    This network starts with a modified ResNet18 model where the first convolutional layer is adapted for a single-channel input.\n",
    "    It includes two additional residual blocks, followed by fully connected layers for classification.\n",
    "\n",
    "    Attributes:\n",
    "        initial_layers (torch.nn.Sequential): The initial convolutional layers, including the modified ResNet layers.\n",
    "        residual_block1 (ResidualBlock): The first residual block with 64 output channels.\n",
    "        residual_block2 (ResidualBlock): The second residual block with 128 output channels and stride 2.\n",
    "        fc1 (torch.nn.Linear): The first fully connected layer.\n",
    "        fc2 (torch.nn.Linear): The second fully connected layer.\n",
    "    \n",
    "    Methods:\n",
    "        forward(x):\n",
    "            Defines the forward pass of the network.\n",
    "        \n",
    "        _get_conv_output(shape):\n",
    "            Helper function to determine the size of the input to the fully connected layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the network layers, including a modified ResNet18 backbone, additional residual blocks, \n",
    "        and fully connected layers.\n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        \n",
    "        self.initial_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),  # Change input channels to 1\n",
    "            resnet.bn1,\n",
    "            resnet.relu,\n",
    "            resnet.maxpool\n",
    "        )\n",
    "\n",
    "        self.initial_layers[0].weight.data = resnet.conv1.weight.data.mean(dim=1, keepdim=True)\n",
    "\n",
    "        self.residual_block1 = ResidualBlock(64, 64)\n",
    "        self.residual_block2 = ResidualBlock(64, 128, stride=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * 7 * 7, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor with shape (N, 1, H, W), where N is the batch size, \n",
    "                              and H and W are the height and width of the input images.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor after passing through the network, with log softmax applied.\n",
    "        \"\"\"\n",
    "        x = self.initial_layers(x)\n",
    "        x = self.residual_block1(x)\n",
    "        x = self.residual_block2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def _get_conv_output(self, shape):\n",
    "        \"\"\"\n",
    "        Helper function to dynamically determine the input size for the fully connected layer.\n",
    "\n",
    "        Args:\n",
    "            shape (tuple): The shape of the input tensor to compute the output size for the fully connected layer.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of features after the convolutional layers and residual blocks.\n",
    "        \"\"\"\n",
    "        x = torch.rand(shape)\n",
    "        x = self.initial_layers(x)\n",
    "        x = self.residual_block1(x)\n",
    "        x = self.residual_block2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x.size(1)\n",
    "\n",
    "def fit(epoch, model, data_loader, phase='training'):\n",
    "    \"\"\"\n",
    "    Trains or validates the model for one epoch.\n",
    "\n",
    "    This function performs either training or validation of the model depending on the specified phase. \n",
    "    It calculates the loss and accuracy for each batch in the data loader and prints the results at the end of the epoch.\n",
    "\n",
    "    Args:\n",
    "        epoch (int): The current epoch number.\n",
    "        model (torch.nn.Module): The model to be trained or validated.\n",
    "        data_loader (torch.utils.data.DataLoader): The data loader providing the data and target batches.\n",
    "        phase (str, optional): The phase of the training process, either 'training' or 'validation'. Defaults to 'training'.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - loss (float): The average loss over the dataset.\n",
    "            - accuracy (float): The accuracy of the model on the dataset as a percentage.\n",
    "    \"\"\"\n",
    "    if phase == 'training':\n",
    "        model.train()\n",
    "    elif phase == 'validation':\n",
    "        model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    \n",
    "    with torch.no_grad() if phase == 'validation' else torch.enable_grad():\n",
    "        for batch_idx, (data, target) in enumerate(data_loader):\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            \n",
    "            if phase == 'training':\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            running_loss += loss.item()\n",
    "            preds = output.data.max(dim=1, keepdim=True)[1]\n",
    "            running_correct += preds.eq(target.data.view_as(preds)).cpu().sum()\n",
    "            \n",
    "            if phase == 'training':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "    loss = running_loss / len(data_loader.dataset)\n",
    "    accuracy = 100.0 * running_correct.item() / len(data_loader.dataset)\n",
    "    \n",
    "    print(f'{phase} loss is {loss:{5}.{2}} and {phase} accuracy is {running_correct}/{len(data_loader.dataset)} {accuracy:{10}.{4}}')\n",
    "    return loss, accuracy\n",
    "\n",
    "model = Net()\n",
    "n_features = model._get_conv_output((1, 1, 28, 28))\n",
    "model.fc1 = nn.Linear(n_features, 50)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "train_losses, train_accuracy = [], []\n",
    "val_losses, val_accuracy = [], []\n",
    "\n",
    "for epoch in range(1, 20):\n",
    "    train_epoch_loss, train_epoch_accuracy = fit(epoch, model, train_loader, phase='training')\n",
    "    val_epoch_loss, val_epoch_accuracy = fit(epoch, model, test_loader, phase='validation')\n",
    "    train_losses.append(train_epoch_loss)\n",
    "    train_accuracy.append(train_epoch_accuracy)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accuracy.append(val_epoch_accuracy)\n",
    "\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, 'bo', label='training')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, 'r', label='validation')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(1, len(train_accuracy) + 1), train_accuracy, 'bo', label='training')\n",
    "plt.plot(range(1, len(val_accuracy) + 1), val_accuracy, 'r', label='validation')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
